{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combffq = pd.read_stata('../Combffq_04Apr19.dta')\n",
    "Combpaq = pd.read_stata('../Combpaq_04Apr19.dta')\n",
    "PEvent = pd.read_stata('../PEvent_04Apr19.dta')\n",
    "nutrition_vars = pd.read_table('nutritionVariables.txt', sep=',')\n",
    "physical_activities = pd.read_table('physicalActivities.txt', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffqcols = ['idno'] + list(nutrition_vars.abbreviation)\n",
    "paqcols = ['idno'] + [_.lower() for _ in physical_activities.abbreviation]\n",
    "merged_data = Combffq[ffqcols].merge(Combpaq[paqcols], left_on='idno', right_on='idno', how='inner')\n",
    "merged_data = merged_data.merge(PEvent[['idno', 'newdiab']], left_on='idno', right_on='idno', how='inner')\n",
    "# null_series = merged_data.isna().sum().sort_values(ascending=False)\n",
    "# too_null_cols = null_series[(null_series>5000)]\n",
    "# DF = merged_data.drop(too_null_cols.index.tolist(), axis=1)\n",
    "DF = merged_data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_train, X_test, y_all_train, y_test = train_test_split(DF.iloc[:,1:-1], DF.iloc[:,-1], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsampling the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diabetic_idx = np.ndarray.flatten(np.array(np.where(y_all_train==1)))\n",
    "# upsampling_idx = np.random.choice(diabetic_idx, len(y_all_train)-2*len(diabetic_idx), replace=True)\n",
    "# sampled_X = pd.concat([X_all_train, X_all_train.iloc[upsampling_idx,]])\n",
    "# sampled_y = pd.concat([y_all_train, y_all_train.iloc[upsampling_idx,]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_idx = np.ndarray.flatten(np.array(np.where(y_all_train==0)))\n",
    "diabetic_idx = np.ndarray.flatten(np.array(np.where(y_all_train==1)))\n",
    "downsample_len = int(len(diabetic_idx)) * 2\n",
    "downsample_ratio = downsample_len / len(healthy_idx) \n",
    "# we are just doing a 50-50! This is why saba's\n",
    "downsampling_idx = np.concatenate((np.random.choice(healthy_idx, downsample_len),np.random.choice(diabetic_idx, downsample_len)), axis=None)\n",
    "np.random.shuffle(downsampling_idx)\n",
    "sampled_X = X_all_train.iloc[downsampling_idx,]\n",
    "sampled_y = y_all_train.iloc[downsampling_idx,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07793619675637141"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsample_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You may verify the effect of downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsampled_X = X_all_train\n",
    "unsampled_y = y_all_train\n",
    "# sampled_X = unsampled_X\n",
    "# sampled_y = unsampled_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-Normalization only based on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio_cols = ['wtredr', 'psfat']\n",
    "# percentage_cols = ['pertotfat', 'persatfat', 'perprotein' ,'percho' ,'permufa' ,'perpufa']\n",
    "# value_cols = list(set(sampled_X.columns)-set(ratio_cols)-set(percentage_cols))\n",
    "# train_mean = sampled_X[value_cols].mean()\n",
    "# train_std = sampled_X[value_cols].std()\n",
    "# sampled_X.loc[:,percentage_cols] = sampled_X.loc[:,percentage_cols]/100\n",
    "# X_test.loc[:,percentage_cols] = X_test.loc[:,percentage_cols]/100\n",
    "# sampled_X.loc[:,value_cols] = (sampled_X.loc[:,value_cols] - train_mean)/train_std\n",
    "# X_test.loc[:,value_cols] = (X_test.loc[:,value_cols] - train_mean)/train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min Max Scaling only based on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnp\\AppData\\Local\\conda\\conda\\envs\\pure_diabetes\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "<ipython-input-10-627e78029056>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sampled_X.loc[:,percentage_cols] = sampled_X.loc[:,percentage_cols]/100\n",
      "<ipython-input-10-627e78029056>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sampled_X.loc[:,value_cols] = (sampled_X.loc[:,value_cols] - train_min)/(train_max-train_min)\n"
     ]
    }
   ],
   "source": [
    "ratio_cols = ['wtredr', 'psfat']\n",
    "percentage_cols = ['pertotfat', 'persatfat', 'perprotein' ,'percho' ,'permufa' ,'perpufa']\n",
    "value_cols = list(set(sampled_X.columns)-set(ratio_cols)-set(percentage_cols))\n",
    "train_max = sampled_X.loc[:,value_cols].max()\n",
    "train_min = sampled_X.loc[:,value_cols].min()\n",
    "sampled_X.loc[:,percentage_cols] = sampled_X.loc[:,percentage_cols]/100\n",
    "X_test.loc[:,percentage_cols] = X_test.loc[:,percentage_cols]/100\n",
    "sampled_X.loc[:,value_cols] = (sampled_X.loc[:,value_cols] - train_min)/(train_max-train_min)\n",
    "X_test.loc[:,value_cols] = (X_test.loc[:,value_cols] - train_min)/(train_max-train_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting train into train and validation for the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(sampled_X, sampled_y, test_size=0.2)\n",
    "# it is important: we need to hold the split correct. That is, we should not do a test split from the sampled data only. \n",
    "# Saba does this correctly, these are *_val variables\n",
    "# X_train, X_val, y_train, y_val = train_test_split(sampled_X, sampled_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23329  4658]\n",
      " [  750   412]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.83      0.90     27987\n",
      "         1.0       0.08      0.35      0.13      1162\n",
      "\n",
      "    accuracy                           0.81     29149\n",
      "   macro avg       0.53      0.59      0.51     29149\n",
      "weighted avg       0.93      0.81      0.87     29149\n",
      "\n",
      "0.594063342785103\n"
     ]
    }
   ],
   "source": [
    "# should do it on the correct splits! \n",
    "rfClassifier = RandomForestClassifier()\n",
    "rfClassifier.fit(X_train, y_train)\n",
    "rf_y_pred = rfClassifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, rf_y_pred))\n",
    "print(classification_report(y_test, rf_y_pred))\n",
    "print(roc_auc_score(y_test, rf_y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsampling - with upweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129463    1.0\n",
       "9234      0.0\n",
       "133872    1.0\n",
       "104923    0.0\n",
       "136082    1.0\n",
       "         ... \n",
       "74458     1.0\n",
       "36373     0.0\n",
       "134658    1.0\n",
       "41859     0.0\n",
       "103053    0.0\n",
       "Name: newdiab, Length: 13993, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we should upweight the examples\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19722  8265]\n",
      " [  597   565]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.70      0.82     27987\n",
      "         1.0       0.06      0.49      0.11      1162\n",
      "\n",
      "    accuracy                           0.70     29149\n",
      "   macro avg       0.52      0.60      0.46     29149\n",
      "weighted avg       0.93      0.70      0.79     29149\n",
      "\n",
      "0.5954574772759936\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(sampled_X, sampled_y, test_size=0.2)\n",
    "# should do it on the correct splits! \n",
    "rfClassifier = RandomForestClassifier(class_weight={0: 1/downsample_ratio, 1:1})\n",
    "rfClassifier.fit(X_train, y_train, )\n",
    "rf_y_pred = rfClassifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, rf_y_pred))\n",
    "print(classification_report(y_test, rf_y_pred))\n",
    "print(roc_auc_score(y_test, rf_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_forest(blen, bcount, X_train, y_train):\n",
    "    all_idx = range(len(X_train))\n",
    "    batch_idx = [np.random.choice(all_idx, blen, replace=True) for _ in range(bcount)]\n",
    "    model = RandomForestClassifier(warm_start=True, n_estimators=1)\n",
    "    for b in batch_idx:\n",
    "        X_batch = X_train.iloc[b,]\n",
    "        y_batch = y_train.iloc[b,]\n",
    "        model.fit(X_batch, y_batch)\n",
    "        model.n_estimators = model.n_estimators + 1\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    return roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc, best_len, best_count = [0, 2000, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing 100 batches, each with size equal to  1000 :\n",
      "\t area under the curve:  0.6588388358168187\n",
      "testing 300 batches, each with size equal to  1000 :\n",
      "\t area under the curve:  0.6653992755711429\n",
      "testing 500 batches, each with size equal to  1000 :\n",
      "\t area under the curve:  0.6622910797753918\n",
      "testing 700 batches, each with size equal to  1000 :\n"
     ]
    }
   ],
   "source": [
    "batch_lens = range(1000,2300, 300)\n",
    "batch_counts = [100, 300, 500, 700]\n",
    "for l in  batch_lens:\n",
    "#     print(\"testing batches with size equal to \", l)\n",
    "    for c in batch_counts:\n",
    "        print(\"testing\", str(c), \"batches, each with size equal to \", str(l), \":\")\n",
    "        model = build_forest(l, c, X_train, y_train)\n",
    "        res = evaluate(model, X_val, y_val)\n",
    "        print(\"\\t area under the curve: \", str(res))\n",
    "        if res > best_auc:\n",
    "            best_auc = res\n",
    "            best_len = l\n",
    "            best_count = c\n",
    "#     print(\"best auc so far: \", best_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_forest(best_len, best_count, sampled_X, sampled_y)\n",
    "y_pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_auc, best_count, best_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_forest(best_len, best_count, X_train, y_train)\n",
    "# y_pred = model.predict(X_val)\n",
    "# print(confusion_matrix(y_val, y_pred))\n",
    "# print(classification_report(y_val, y_pred))\n",
    "# print(roc_auc_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(X_test)\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfClassifier = RandomForestClassifier()\n",
    "rfClassifier.fit(sampled_X, sampled_y)\n",
    "rf_y_pred = rfClassifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, rf_y_pred))\n",
    "print(classification_report(y_test, rf_y_pred))\n",
    "print(roc_auc_score(y_test, rf_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should do it on the correct splits! \n",
    "rfClassifier = RandomForestClassifier()\n",
    "rfClassifier.fit(X_train, y_train)\n",
    "rf_y_pred = rfClassifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, rf_y_pred))\n",
    "print(classification_report(y_test, rf_y_pred))\n",
    "print(roc_auc_score(y_test, rf_y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnClassifier = KNeighborsClassifier(n_neighbors=2)\n",
    "knnClassifier.fit(sampled_X, sampled_y)\n",
    "knn_y_pred = knnClassifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, knn_y_pred))\n",
    "print(classification_report(y_test, knn_y_pred))\n",
    "print(roc_auc_score(y_test, knn_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtClassifier = DecisionTreeClassifier()\n",
    "dtClassifier.fit(sampled_X, sampled_y)\n",
    "dt_y_pred = dtClassifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, dt_y_pred))\n",
    "print(classification_report(y_test, dt_y_pred))\n",
    "print(roc_auc_score(y_test, dt_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_pred = np.zeros(len(rf_y_pred))\n",
    "for i in range(len(rf_y_pred)):\n",
    "    if rf_y_pred[i]+knn_y_pred[i]+dt_y_pred[i] >= 2:\n",
    "        voting_pred[i] = 1\n",
    "print(confusion_matrix(y_test, voting_pred))\n",
    "print(classification_report(y_test, voting_pred))\n",
    "print(roc_auc_score(y_test, voting_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = DF.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(20,20))\n",
    "g=sns.heatmap(DF[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n",
    "plt.savefig('corr.png', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "sel = SelectFromModel(model)\n",
    "sel.fit(sampled_X, sampled_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_X.columns[(sel.get_support())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.series(sel.estimator_,feature_importances_.ravel()).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sel.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ffqcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paqcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_data.columns), len(DF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-pure_diabetes] *",
   "language": "python",
   "name": "conda-env-conda-pure_diabetes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
